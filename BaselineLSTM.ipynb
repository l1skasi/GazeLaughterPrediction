{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ttqGtd8uo8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-z3C1mVNRE_",
        "outputId": "034c4959-d32d-4e43-86f7-45e0c0edc728"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-14 23:35:57.148033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2025-08-14 23:35:57.148059: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzrWcvFQRo4o",
        "outputId": "dace8527-9e1d-4b6d-a9fe-7a99c65fc0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q95l9eIhUWrT"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OBHzIOlUbxz"
      },
      "source": [
        "## Retrieve annotations by seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr5SAm2GFWxR"
      },
      "outputs": [],
      "source": [
        "file_name = \"A_12_20130925_1212_01_2.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnZV73RpHp6a",
        "outputId": "5371cd33-4bd7-49a5-d28f-e809463168ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-second annotations saved to /home/elizaveta/Documents/RedBallGame/annotations_per_t.csv\n"
          ]
        }
      ],
      "source": [
        "def load_annotations(file_name: str):\n",
        "    df_raw = pd.read_csv(file_name, delimiter='\\t', header=None)\n",
        "    df = df_raw.iloc[:, [0, 2, 4, 6, 8]].copy()\n",
        "    df.columns = ['Tier_Name', 'Start_Time', 'End_Time', 'Duration', 'Annotation_Value']\n",
        "    return df\n",
        "\n",
        "def filter_by_scene(df: pd.DataFrame):\n",
        "    scene_records = df[df['Tier_Name'] == 'Scene']\n",
        "    if scene_records.empty:\n",
        "        print(\"Scene tier not found. Keeping all annotations.\")\n",
        "        return df\n",
        "\n",
        "    # Retrieve the scene intervals\n",
        "    scene_intervals = []\n",
        "    for index, row in scene_records.iterrows():\n",
        "        scene_intervals.append((row['Start_Time'], row['End_Time']))\n",
        "\n",
        "    # Filter annotations based on these intervals\n",
        "    df_filtered = pd.DataFrame()\n",
        "    for start_time, end_time in scene_intervals:\n",
        "        df_fragment = df[(df['Start_Time'] >= start_time) & (df['End_Time'] <= end_time)].copy()\n",
        "        df_filtered = pd.concat([df_filtered, df_fragment])\n",
        "\n",
        "    df = df_filtered.copy()\n",
        "    return df\n",
        "\n",
        "def time_string_to_ms(time_str: str):\n",
        "    # Convert time string to milliseconds\n",
        "    if pd.isna(time_str):\n",
        "        return None\n",
        "    h, m, s_ms = time_str.split(':')\n",
        "    s, ms = s_ms.split('.')\n",
        "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)\n",
        "\n",
        "def preprocess_times(df: pd.DataFrame):\n",
        "    df = df.copy()\n",
        "    df['Start_Time_ms'] = df['Start_Time'].apply(time_string_to_ms)\n",
        "    df['End_Time_ms'] = df['End_Time'].apply(time_string_to_ms)\n",
        "    df['Start'] = (df['Start_Time_ms'] // 100).astype('Int64')\n",
        "    df['End'] = (df['End_Time_ms'] // 100).astype('Int64')\n",
        "\n",
        "    df = df.dropna()\n",
        "\n",
        "    for c in ['Tier_Name', 'Annotation_Value']:\n",
        "        df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "    #Remove reference numbers in annotations, e.g. \"14_mom spits the ball out of the mouth\"\n",
        "    df['Annotation_Value'] = df['Annotation_Value'].apply(lambda x: re.sub(r'^\\d+_', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "def annotations_per_s(df: pd.DataFrame):\n",
        "    rows = []\n",
        "    max_t = int(df['End'].max())\n",
        "\n",
        "    for t in range(0, max_t + 1):\n",
        "        active_rows = df[(df['Start'] <= t) & (df['End'] > t)]\n",
        "        if not active_rows.empty:\n",
        "            for _, row in active_rows.iterrows():\n",
        "              if row['Tier_Name'] not in [\"Round\", \"Scene\", \"Comment\"]:\n",
        "                rows.append({'Time': t, 'Tier': row['Tier_Name'], 'Annotation': row['Annotation_Value']})\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def export(df: pd.DataFrame, output_name: str):\n",
        "    df = df.dropna()\n",
        "    df.to_csv(output_name, index=False)\n",
        "    print(f\"Per-second annotations saved to {Path(output_name).resolve()}\")\n",
        "\n",
        "\n",
        "def process_annotations(file_name: str, output_name: str = \"annotations_per_t.csv\"):\n",
        "    df = load_annotations(file_name)\n",
        "    df = filter_by_scene(df)\n",
        "    df = preprocess_times(df)\n",
        "    per_t_df = annotations_per_s(df)\n",
        "    export(per_t_df, output_name)\n",
        "\n",
        "\n",
        "process_annotations(file_name, \"annotations_per_t.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v3UAUx1L5Yw"
      },
      "source": [
        "## Annotations categorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZJjoY_-jOd8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"annotations_per_t.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH76tlCnTbd9",
        "outputId": "8d041b8c-57a2-4b8b-d874-ae693bb07041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotation\n",
            "invisible              1302\n",
            "neutral                 503\n",
            "smile showing teeth     436\n",
            "widely opened eyes      208\n",
            "smile                   197\n",
            "O-shaped mouth          104\n",
            "biting lower lip         18\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Categorize Facial@MOT annotations\n",
        "\n",
        "facial_mot_counts = df[df['Tier']=='Facial@MOT'].value_counts('Annotation')\n",
        "print(facial_mot_counts)\n",
        "\n",
        "facial_mapping = {\n",
        "    \"smile showing teeth\": \"positive\",\n",
        "    \"smile\": \"positive\",\n",
        "    \"widely opened eyes\": \"positive\",\n",
        "    \"O-shaped mouth\": \"neutral\",\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"biting lower lip\": \"neutral\",\n",
        "    \"invisible\": \"invisible\"\n",
        "}\n",
        "\n",
        "def categorize_facial_mot(facial_annotation):\n",
        "    return facial_mapping.get(facial_annotation, \"neutral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4iLnFSyBNaM",
        "outputId": "29a029d8-5a05-47bf-9af3-440340e828a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotation\n",
            "She’s laughing so hard that she literally can’t even stay seated.    66\n",
            "This is harmonious to hear.                                          24\n",
            "What even set off you laughing so hard?                              22\n",
            "I hope this is not the laugh before the cry.                         22\n",
            "Ready?                                                               18\n",
            "... tickle you                                                       15\n",
            "chooga                                                               15\n",
            "Yeaaah!                                                              12\n",
            "Keep falling.                                                        11\n",
            "Can I please have a ball?                                             9\n",
            "Really far.                                                           9\n",
            "Drop it.                                                              9\n",
            "I love you.                                                           8\n",
            "Can you sit up?                                                       7\n",
            "Put it in here.                                                       7\n",
            "Drop it in.                                                           6\n",
            "I take this one.                                                      5\n",
            "Too far.                                                              5\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Categorize Utterance@MOT annotations\n",
        "utterance_mot_counts = df[df['Tier']=='Utterance@MOT'].value_counts('Annotation')\n",
        "print(utterance_mot_counts)\n",
        "\n",
        "# Utterance categories\n",
        "questions = [\n",
        "    \"Ready?\",\n",
        "    \"Can I please have a ball?\",\n",
        "    \"What even set off you laughing so hard?\"\n",
        "]\n",
        "commands = [\n",
        "    \"Drop it.\",\n",
        "    \"Drop it in.\",\n",
        "    \"Put it in here.\",\n",
        "    \"Can you sit up?\",\n",
        "]\n",
        "affection = [\n",
        "    \"I love you.\"\n",
        "]\n",
        "\n",
        "playful_sounds = [\n",
        "    \"chooga\",\n",
        "    \"... tickle you\",\n",
        "    \"Yeaaah!\"\n",
        "]\n",
        "\n",
        "statements = [\n",
        "    \"She’s laughing so hard that she literally can’t even stay seated.\",\n",
        "    \"I hope this is not the laugh before the cry.\",\n",
        "    \"Really far.\",\n",
        "    \"Too far.\",\n",
        "    \"I take this one.\",\n",
        "    \"This is harmonious to hear.\",\n",
        "    \"Keep falling.\"\n",
        "]\n",
        "\n",
        "utterance_lists = {\n",
        "    'question': questions,\n",
        "    'statement': statements,\n",
        "    'affection': affection,\n",
        "    'playful_sounds': playful_sounds\n",
        "}\n",
        "\n",
        "utterance_category_mapping = {value: category for category, values in utterance_lists.items() for value in values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLx3N-gzB2wZ"
      },
      "outputs": [],
      "source": [
        "def categorize_utterance_mot(value):\n",
        "    return utterance_category_mapping.get(value, \"statement\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiJh12sbhtib"
      },
      "outputs": [],
      "source": [
        "df['Annotation'] = df.apply(\n",
        "    lambda row: categorize_facial_mot(row['Annotation']) if row['Tier'] == 'Facial@MOT' else\n",
        "                categorize_utterance_mot(row['Annotation']) if row['Tier'] == 'Utterance@MOT' else\n",
        "                row['Annotation'],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5x_Plz0Ufkb"
      },
      "source": [
        "## Create objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCn9ZuzKmMgB"
      },
      "outputs": [],
      "source": [
        "tiers = [\n",
        "    \"Action@MOT\",\n",
        "    \"Gaze@MOT\",\n",
        "    \"Utterance@MOT\",\n",
        "    \"Prosody@MOT\",\n",
        "    \"Facial@MOT\",\n",
        "    \"Laughter@MOT\",\n",
        "    \"Laughter@CHI\",\n",
        "    \"Gaze@CHI\",\n",
        "    \"GazePattern\"\n",
        "]\n",
        "\n",
        "pivot_df = df.pivot_table(\n",
        "    index='Time',\n",
        "    columns='Tier',\n",
        "    values='Annotation',\n",
        "    aggfunc=lambda x: x.iloc[0]  # if multiple rows for same tier & second, take first\n",
        ")\n",
        "\n",
        "# Create GazePattern column before selecting tiers\n",
        "records = pivot_df.to_dict(orient='records')\n",
        "for record in records:\n",
        "    mot_target = record.get(\"Gaze@MOT\")\n",
        "    chi_target = record.get(\"Gaze@CHI\")\n",
        "\n",
        "    if chi_target == \"mom\" and mot_target == \"child\":\n",
        "        label = \"MA\"  # MutualAttention\n",
        "    elif mot_target == \"invisible\":\n",
        "        label = \"<UNK>\"  # Unknown\n",
        "    elif chi_target == \"mom\":\n",
        "        label = \"SA(CHI)\"  # SingleAttention(CHI)\n",
        "    elif mot_target == \"child\":\n",
        "        label = \"SA(MOT)\"  # SingleAttention(MOT)\n",
        "    elif chi_target == mot_target and chi_target is not None:\n",
        "        label = \"ShA\"  # SharedAttention\n",
        "    elif chi_target != mot_target:\n",
        "        label = \"L\"  # LostAttention\n",
        "    else:\n",
        "        label = \"<UNK>\"  # Unknown\n",
        "\n",
        "    record[\"GazePattern\"] = label\n",
        "\n",
        "# Convert records back to DataFrame to select tiers\n",
        "pivot_df = pd.DataFrame(records)\n",
        "\n",
        "pivot_df = pivot_df[tiers]\n",
        "\n",
        "pivot_df = pivot_df.where(pd.notnull(pivot_df), None)\n",
        "\n",
        "records = pivot_df.to_dict(orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Hd1RW2lffb"
      },
      "outputs": [],
      "source": [
        "X = records.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4_xX2KrVCx"
      },
      "source": [
        "## Retrieve vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0asYcrN3rY1c"
      },
      "outputs": [],
      "source": [
        "values = defaultdict(set)\n",
        "\n",
        "for r in records:\n",
        "    for key in r.keys():\n",
        "        value = r[key]\n",
        "        if value is None or value == '' or value == 'invisible':\n",
        "            value = \"<UNK>\"  # use <UNK> token for missing\n",
        "        values[key].add(value)\n",
        "\n",
        "vocab_action    = {token: idx for idx, token in enumerate(sorted(values['Action@MOT']))}\n",
        "vocab_gaze      = {token: idx for idx, token in enumerate(sorted(values['Gaze@MOT']))}\n",
        "vocab_utterance = {token: idx for idx, token in enumerate(sorted(values['Utterance@MOT']))}\n",
        "vocab_prosody   = {token: idx for idx, token in enumerate(sorted(values['Prosody@MOT']))}\n",
        "vocab_facial    = {token: idx for idx, token in enumerate(sorted(values['Facial@MOT']))}\n",
        "vocab_laughter  = {token: idx for idx, token in enumerate(sorted(values['Laughter@MOT']))}\n",
        "vocab_laughter_chi = {token: idx for idx, token in enumerate(sorted(values['Laughter@CHI']))}\n",
        "vocab_gaze_chi  = {token: idx for idx, token in enumerate(sorted(values['Gaze@CHI']))}\n",
        "vocab_gazerelation  = {token: idx for idx, token in enumerate(sorted(values['GazePattern']))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fnvlc4sUiJD"
      },
      "source": [
        "## Divide dataset on train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAwcv6B6azju"
      },
      "outputs": [],
      "source": [
        "# timeline_length = len(records)\n",
        "# train_end = int(0.75 * timeline_length)\n",
        "# validation_end = int(0.90 * timeline_length) # 75% for train, 15% for validation, 10% for test\n",
        "\n",
        "# X_train = X[:train_end]\n",
        "# X_validation = X[train_end:validation_end]\n",
        "# X_test  = X[validation_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtLa-uHxTX1"
      },
      "outputs": [],
      "source": [
        "def split_datset (X, n_parts, train_size, val_size):\n",
        "    n_parts = 4\n",
        "\n",
        "    n = len(X)\n",
        "    part_size = n // n_parts\n",
        "\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "\n",
        "    for i in range(n_parts):\n",
        "        start = i * part_size\n",
        "        end = (i + 1) * part_size if i < n_parts - 1 else n\n",
        "\n",
        "        part_indices = np.arange(start, end)\n",
        "\n",
        "        n_train = int(len(part_indices) * train_size)\n",
        "        n_val = int(len(part_indices) * val_size)\n",
        "        # n_test = len(part_indices) - n_train - n_val  # остаток\n",
        "\n",
        "        # Индексы для этой части (сохраняем порядок)\n",
        "        train_idx.extend(part_indices[:n_train])\n",
        "        val_idx.extend(part_indices[n_train:n_train + n_val])\n",
        "        test_idx.extend(part_indices[n_train + n_val:])\n",
        "\n",
        "    # Собираем выборки\n",
        "    X_train = [X[i] for i in train_idx]\n",
        "    X_validation = [X[i] for i in val_idx]\n",
        "    X_test = [X[i] for i in test_idx]\n",
        "\n",
        "    return X_train, X_validation, X_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB1o0ec2xWxf",
        "outputId": "db784179-1588-445c-8b67-f50379a6b392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2326\n",
            "496\n",
            "504\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_validation))\n",
        "print(len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovJ9MT28inm6"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koms8UCwGxfh"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7cpD08fipMS"
      },
      "outputs": [],
      "source": [
        "class SocialActionPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 # passing the vocabularies for embeddings\n",
        "                 vocab_gaze, vocab_utterance, vocab_prosody,\n",
        "                 vocab_facial, vocab_laughter, vocab_action,\n",
        "                 vocab_laughter_chi, vocab_gaze_chi,\n",
        "                 hidden_dim = 16\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        # Vocabs\n",
        "        self.vocab_gaze = vocab_gaze\n",
        "        self.vocab_utterance = vocab_utterance\n",
        "        self.vocab_prosody = vocab_prosody\n",
        "        self.vocab_facial = vocab_facial\n",
        "        self.vocab_laughter = vocab_laughter\n",
        "        self.vocab_action = vocab_action\n",
        "        self.vocab_laughter_chi = vocab_laughter_chi\n",
        "        self.vocab_gaze_chi = vocab_gaze_chi\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embed_gaze = nn.Embedding(len(vocab_gaze), 3) # nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embed_utterance = nn.Embedding(len(vocab_utterance), 3)\n",
        "        self.embed_prosody = nn.Embedding(len(vocab_prosody), 2)\n",
        "        self.embed_facial = nn.Embedding(len(vocab_facial), 3)\n",
        "        self.embed_laughter = nn.Embedding(len(vocab_laughter), 2)\n",
        "        self.embed_action = nn.Embedding(len(vocab_action), 3)\n",
        "\n",
        "        self.embed_laughter_chi = nn.Embedding(len(vocab_laughter_chi), 2)\n",
        "        self.embed_gaze_chi = nn.Embedding(len(vocab_gaze_chi), 3)\n",
        "\n",
        "        # LSTM\n",
        "        input_dim = 3 + 3 + 2 + 3 + 2 + 3 # considering only mother's tiers\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm_cell = nn.LSTMCell(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Decoder to predict child gaze, laughter and mother's action\n",
        "        self.decoder_gaze = nn.Linear(hidden_dim, len(vocab_gaze_chi))\n",
        "        self.decoder_laughter = nn.Linear(hidden_dim, len(vocab_laughter_chi))\n",
        "\n",
        "    # method for finding value's index in list\n",
        "    def lookup(self, vocab, value):\n",
        "        if value is None:\n",
        "            value = \"<UNK>\"\n",
        "        return vocab.get(value, vocab[\"<UNK>\"])\n",
        "\n",
        "    # converting inputs into indices\n",
        "    def encode_input(self, input, device):\n",
        "        gaze = torch.tensor([self.lookup(self.vocab_gaze, input.get(\"Gaze@MOT\"))]).to(device)\n",
        "        utterance = torch.tensor([self.lookup(self.vocab_utterance, input.get(\"Utterance@MOT\"))]).to(device)\n",
        "        prosody = torch.tensor([self.lookup(self.vocab_prosody, input.get(\"Prosody@MOT\"))]).to(device)\n",
        "        facial = torch.tensor([self.lookup(self.vocab_facial, input.get(\"Facial@MOT\"))]).to(device)\n",
        "        laughter = torch.tensor([self.lookup(self.vocab_laughter, input.get(\"Laughter@MOT\"))]).to(device)\n",
        "        action = torch.tensor([self.lookup(self.vocab_action, input.get(\"Action@MOT\"))]).to(device)\n",
        "\n",
        "        # concatenating the embeddings from all modalities\n",
        "        return torch.cat([\n",
        "            self.embed_gaze(gaze),\n",
        "            self.embed_utterance(utterance),\n",
        "            self.embed_prosody(prosody),\n",
        "            self.embed_facial(facial),\n",
        "            self.embed_laughter(laughter),\n",
        "            self.embed_action(action)\n",
        "        ], dim=-1)\n",
        "\n",
        "    def forward(self, sequence_of_dicts, device):\n",
        "        # initializating short (h) and long (c) term memory\n",
        "        h_t = torch.zeros(1, self.hidden_dim).to(device)\n",
        "        c_t = torch.zeros(1, self.hidden_dim).to(device)\n",
        "\n",
        "        # predicted by LSTM cell\n",
        "        logits_gaze_per_step = []\n",
        "        logits_laughter_per_step = []\n",
        "\n",
        "        # expected results\n",
        "        targets_gaze_per_step = []\n",
        "        targets_laughter_per_step = []\n",
        "\n",
        "        for step_dict in sequence_of_dicts:\n",
        "            x_t = self.encode_input(step_dict, device) # input\n",
        "            h_t, c_t = self.lstm_cell(x_t, (h_t, c_t)) # short and long term memory\n",
        "\n",
        "            # Predict child gaze and laughter\n",
        "            logits_gaze = self.decoder_gaze(h_t)\n",
        "            logits_laughter = self.decoder_laughter(h_t)\n",
        "\n",
        "            # These are gaze and laughter that are predicted by LSTM\n",
        "            logits_gaze_per_step.append(logits_gaze)\n",
        "            logits_laughter_per_step.append(logits_laughter)\n",
        "\n",
        "            # Store actual child gaze and laughter indices for loss calculation\n",
        "            targets_gaze_per_step.append(self.lookup(self.vocab_gaze_chi, step_dict.get(\"Gaze@CHI\")))\n",
        "            targets_laughter_per_step.append(self.lookup(self.vocab_laughter_chi, step_dict.get(\"Laughter@CHI\")))\n",
        "\n",
        "\n",
        "        # Concatenate the list of tensors along the first dimension (sequence length)\n",
        "        logits_gaze_per_step = torch.cat(logits_gaze_per_step, dim=0)\n",
        "        logits_laughter_per_step = torch.cat(logits_laughter_per_step, dim=0)\n",
        "\n",
        "        # Convert the list of target indices to single tensors\n",
        "        targets_gaze_per_step = torch.tensor(targets_gaze_per_step, dtype=torch.long).to(device)\n",
        "        targets_laughter_per_step = torch.tensor(targets_laughter_per_step, dtype=torch.long).to(device)\n",
        "\n",
        "        return (logits_gaze_per_step, # predicted with LSTM\n",
        "                logits_laughter_per_step, # predicted with LSTM\n",
        "                targets_gaze_per_step, # expected (from input)\n",
        "                targets_laughter_per_step # expected (from input)\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4LrOoYgGz7E"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIS417Z-ma3T",
        "outputId": "eb13ca9c-20c9-40a2-c559-f4e300c928b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch [1/50], Train Loss: 2.2623, Train Gaze Acc: 0.1836, Train Laughter Acc: 0.6580, Val Loss: 2.2567, Val Gaze Acc: 0.0240, Val Laughter Acc: 0.8176\n",
            "Epoch [2/50], Train Loss: 2.2564, Train Gaze Acc: 0.1832, Train Laughter Acc: 0.6580, Val Loss: 2.2521, Val Gaze Acc: 0.0240, Val Laughter Acc: 0.8176\n",
            "Epoch [3/50], Train Loss: 2.2505, Train Gaze Acc: 0.1848, Train Laughter Acc: 0.6620, Val Loss: 2.2476, Val Gaze Acc: 0.0261, Val Laughter Acc: 0.8176\n",
            "Epoch [4/50], Train Loss: 2.2447, Train Gaze Acc: 0.1848, Train Laughter Acc: 0.6672, Val Loss: 2.2432, Val Gaze Acc: 0.0281, Val Laughter Acc: 0.8196\n",
            "Epoch [5/50], Train Loss: 2.2389, Train Gaze Acc: 0.1885, Train Laughter Acc: 0.6692, Val Loss: 2.2388, Val Gaze Acc: 0.0281, Val Laughter Acc: 0.8357\n",
            "Epoch [6/50], Train Loss: 2.2332, Train Gaze Acc: 0.1937, Train Laughter Acc: 0.6708, Val Loss: 2.2345, Val Gaze Acc: 0.0281, Val Laughter Acc: 0.8357\n",
            "Epoch [7/50], Train Loss: 2.2275, Train Gaze Acc: 0.1981, Train Laughter Acc: 0.6756, Val Loss: 2.2303, Val Gaze Acc: 0.0301, Val Laughter Acc: 0.8397\n",
            "Epoch [8/50], Train Loss: 2.2219, Train Gaze Acc: 0.2037, Train Laughter Acc: 0.6792, Val Loss: 2.2262, Val Gaze Acc: 0.0301, Val Laughter Acc: 0.8477\n",
            "Epoch [9/50], Train Loss: 2.2163, Train Gaze Acc: 0.2085, Train Laughter Acc: 0.6852, Val Loss: 2.2221, Val Gaze Acc: 0.0381, Val Laughter Acc: 0.8537\n",
            "Epoch [10/50], Train Loss: 2.2108, Train Gaze Acc: 0.2161, Train Laughter Acc: 0.6961, Val Loss: 2.2180, Val Gaze Acc: 0.0381, Val Laughter Acc: 0.8577\n",
            "Epoch [11/50], Train Loss: 2.2053, Train Gaze Acc: 0.2209, Train Laughter Acc: 0.6985, Val Loss: 2.2140, Val Gaze Acc: 0.0401, Val Laughter Acc: 0.8597\n",
            "Epoch [12/50], Train Loss: 2.1998, Train Gaze Acc: 0.2249, Train Laughter Acc: 0.6993, Val Loss: 2.2101, Val Gaze Acc: 0.0441, Val Laughter Acc: 0.8617\n",
            "Epoch [13/50], Train Loss: 2.1943, Train Gaze Acc: 0.2334, Train Laughter Acc: 0.6993, Val Loss: 2.2063, Val Gaze Acc: 0.0441, Val Laughter Acc: 0.8637\n",
            "Epoch [14/50], Train Loss: 2.1888, Train Gaze Acc: 0.2903, Train Laughter Acc: 0.7001, Val Loss: 2.2026, Val Gaze Acc: 0.0481, Val Laughter Acc: 0.8637\n",
            "Epoch [15/50], Train Loss: 2.1834, Train Gaze Acc: 0.3023, Train Laughter Acc: 0.7005, Val Loss: 2.1989, Val Gaze Acc: 0.0521, Val Laughter Acc: 0.8657\n",
            "Epoch [16/50], Train Loss: 2.1780, Train Gaze Acc: 0.3003, Train Laughter Acc: 0.6981, Val Loss: 2.1954, Val Gaze Acc: 0.0541, Val Laughter Acc: 0.8697\n",
            "Epoch [17/50], Train Loss: 2.1725, Train Gaze Acc: 0.3123, Train Laughter Acc: 0.6965, Val Loss: 2.1919, Val Gaze Acc: 0.0561, Val Laughter Acc: 0.8697\n",
            "Epoch [18/50], Train Loss: 2.1671, Train Gaze Acc: 0.3256, Train Laughter Acc: 0.6949, Val Loss: 2.1885, Val Gaze Acc: 0.0581, Val Laughter Acc: 0.8697\n",
            "Epoch [19/50], Train Loss: 2.1617, Train Gaze Acc: 0.3356, Train Laughter Acc: 0.6961, Val Loss: 2.1852, Val Gaze Acc: 0.0581, Val Laughter Acc: 0.8697\n",
            "Epoch [20/50], Train Loss: 2.1562, Train Gaze Acc: 0.3460, Train Laughter Acc: 0.6945, Val Loss: 2.1819, Val Gaze Acc: 0.0641, Val Laughter Acc: 0.8697\n",
            "Epoch [21/50], Train Loss: 2.1507, Train Gaze Acc: 0.3549, Train Laughter Acc: 0.6945, Val Loss: 2.1788, Val Gaze Acc: 0.0681, Val Laughter Acc: 0.8697\n",
            "Epoch [22/50], Train Loss: 2.1452, Train Gaze Acc: 0.3841, Train Laughter Acc: 0.6945, Val Loss: 2.1757, Val Gaze Acc: 0.0681, Val Laughter Acc: 0.8697\n",
            "Epoch [23/50], Train Loss: 2.1397, Train Gaze Acc: 0.4218, Train Laughter Acc: 0.6961, Val Loss: 2.1727, Val Gaze Acc: 0.0701, Val Laughter Acc: 0.8717\n",
            "Epoch [24/50], Train Loss: 2.1341, Train Gaze Acc: 0.4370, Train Laughter Acc: 0.6993, Val Loss: 2.1698, Val Gaze Acc: 0.0721, Val Laughter Acc: 0.8717\n",
            "Epoch [25/50], Train Loss: 2.1285, Train Gaze Acc: 0.4559, Train Laughter Acc: 0.7029, Val Loss: 2.1669, Val Gaze Acc: 0.0762, Val Laughter Acc: 0.8677\n",
            "Epoch [26/50], Train Loss: 2.1229, Train Gaze Acc: 0.4619, Train Laughter Acc: 0.7029, Val Loss: 2.1641, Val Gaze Acc: 0.0822, Val Laughter Acc: 0.8637\n",
            "Epoch [27/50], Train Loss: 2.1172, Train Gaze Acc: 0.4707, Train Laughter Acc: 0.7061, Val Loss: 2.1614, Val Gaze Acc: 0.0862, Val Laughter Acc: 0.8657\n",
            "Epoch [28/50], Train Loss: 2.1114, Train Gaze Acc: 0.4840, Train Laughter Acc: 0.7061, Val Loss: 2.1588, Val Gaze Acc: 0.0922, Val Laughter Acc: 0.8637\n",
            "Epoch [29/50], Train Loss: 2.1056, Train Gaze Acc: 0.5068, Train Laughter Acc: 0.7061, Val Loss: 2.1562, Val Gaze Acc: 0.0962, Val Laughter Acc: 0.8637\n",
            "Epoch [30/50], Train Loss: 2.0997, Train Gaze Acc: 0.5293, Train Laughter Acc: 0.7077, Val Loss: 2.1536, Val Gaze Acc: 0.1062, Val Laughter Acc: 0.8617\n",
            "Epoch [31/50], Train Loss: 2.0937, Train Gaze Acc: 0.5453, Train Laughter Acc: 0.7073, Val Loss: 2.1512, Val Gaze Acc: 0.1102, Val Laughter Acc: 0.8617\n",
            "Epoch [32/50], Train Loss: 2.0877, Train Gaze Acc: 0.5545, Train Laughter Acc: 0.7069, Val Loss: 2.1487, Val Gaze Acc: 0.1142, Val Laughter Acc: 0.8637\n",
            "Epoch [33/50], Train Loss: 2.0815, Train Gaze Acc: 0.5722, Train Laughter Acc: 0.7085, Val Loss: 2.1464, Val Gaze Acc: 0.1162, Val Laughter Acc: 0.8637\n",
            "Epoch [34/50], Train Loss: 2.0753, Train Gaze Acc: 0.5850, Train Laughter Acc: 0.7097, Val Loss: 2.1441, Val Gaze Acc: 0.1182, Val Laughter Acc: 0.8617\n",
            "Epoch [35/50], Train Loss: 2.0690, Train Gaze Acc: 0.5890, Train Laughter Acc: 0.7121, Val Loss: 2.1418, Val Gaze Acc: 0.1182, Val Laughter Acc: 0.8617\n",
            "Epoch [36/50], Train Loss: 2.0626, Train Gaze Acc: 0.5922, Train Laughter Acc: 0.7149, Val Loss: 2.1396, Val Gaze Acc: 0.1202, Val Laughter Acc: 0.8597\n",
            "Epoch [37/50], Train Loss: 2.0561, Train Gaze Acc: 0.5934, Train Laughter Acc: 0.7169, Val Loss: 2.1374, Val Gaze Acc: 0.1263, Val Laughter Acc: 0.8597\n",
            "Epoch [38/50], Train Loss: 2.0495, Train Gaze Acc: 0.5942, Train Laughter Acc: 0.7185, Val Loss: 2.1353, Val Gaze Acc: 0.1363, Val Laughter Acc: 0.8597\n",
            "Epoch [39/50], Train Loss: 2.0427, Train Gaze Acc: 0.6010, Train Laughter Acc: 0.7173, Val Loss: 2.1333, Val Gaze Acc: 0.1443, Val Laughter Acc: 0.8597\n",
            "Epoch [40/50], Train Loss: 2.0359, Train Gaze Acc: 0.6018, Train Laughter Acc: 0.7165, Val Loss: 2.1313, Val Gaze Acc: 0.1583, Val Laughter Acc: 0.8637\n",
            "Epoch [41/50], Train Loss: 2.0290, Train Gaze Acc: 0.6026, Train Laughter Acc: 0.7161, Val Loss: 2.1294, Val Gaze Acc: 0.1683, Val Laughter Acc: 0.8677\n",
            "Epoch [42/50], Train Loss: 2.0219, Train Gaze Acc: 0.6055, Train Laughter Acc: 0.7173, Val Loss: 2.1276, Val Gaze Acc: 0.1723, Val Laughter Acc: 0.8677\n",
            "Epoch [43/50], Train Loss: 2.0147, Train Gaze Acc: 0.6063, Train Laughter Acc: 0.7189, Val Loss: 2.1258, Val Gaze Acc: 0.1743, Val Laughter Acc: 0.8677\n",
            "Epoch [44/50], Train Loss: 2.0074, Train Gaze Acc: 0.6079, Train Laughter Acc: 0.7197, Val Loss: 2.1241, Val Gaze Acc: 0.1824, Val Laughter Acc: 0.8657\n",
            "Epoch [45/50], Train Loss: 2.0000, Train Gaze Acc: 0.6095, Train Laughter Acc: 0.7205, Val Loss: 2.1224, Val Gaze Acc: 0.1824, Val Laughter Acc: 0.8657\n",
            "Epoch [46/50], Train Loss: 1.9925, Train Gaze Acc: 0.6099, Train Laughter Acc: 0.7209, Val Loss: 2.1208, Val Gaze Acc: 0.1844, Val Laughter Acc: 0.8677\n",
            "Epoch [47/50], Train Loss: 1.9849, Train Gaze Acc: 0.6107, Train Laughter Acc: 0.7213, Val Loss: 2.1193, Val Gaze Acc: 0.1844, Val Laughter Acc: 0.8697\n",
            "Epoch [48/50], Train Loss: 1.9772, Train Gaze Acc: 0.6115, Train Laughter Acc: 0.7217, Val Loss: 2.1178, Val Gaze Acc: 0.1844, Val Laughter Acc: 0.8697\n",
            "Epoch [49/50], Train Loss: 1.9694, Train Gaze Acc: 0.6119, Train Laughter Acc: 0.7201, Val Loss: 2.1164, Val Gaze Acc: 0.1844, Val Laughter Acc: 0.8697\n",
            "Epoch [50/50], Train Loss: 1.9615, Train Gaze Acc: 0.6119, Train Laughter Acc: 0.7189, Val Loss: 2.1151, Val Gaze Acc: 0.1844, Val Laughter Acc: 0.8717\n",
            "Training finished.\n",
            "\n",
            "Test Set Evaluation:\n",
            "Test Loss: 2.2289, Test Gaze Acc: 0.2492, Test Laughter Acc: 0.7267\n"
          ]
        }
      ],
      "source": [
        "model = SocialActionPredictor(\n",
        "    vocab_gaze, vocab_utterance, vocab_prosody,\n",
        "    vocab_facial, vocab_laughter, vocab_action,\n",
        "    vocab_laughter_chi, vocab_gaze_chi,\n",
        "    hidden_dim = 8,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "# Lists to store metrics per epoch\n",
        "train_losses = []\n",
        "train_gaze_accuracies = []\n",
        "train_laughter_accuracies = []\n",
        "val_losses = []\n",
        "val_gaze_accuracies = []\n",
        "val_laughter_accuracies = []\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training\n",
        "    (logits_gaze, logits_laughter,\n",
        "     targets_gaze, targets_laughter) = model(X_train, device)\n",
        "\n",
        "    # Calculate loss for child gaze and laughter prediction\n",
        "    loss_gaze = criterion(logits_gaze, targets_gaze)\n",
        "    loss_laughter = criterion(logits_laughter, targets_laughter)\n",
        "\n",
        "    # Combine losses\n",
        "    loss = loss_gaze + loss_laughter\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss = loss.item()\n",
        "    train_losses.append(total_loss) # Store training loss\n",
        "\n",
        "    pred_gaze = torch.argmax(logits_gaze, dim=1)\n",
        "    correct_gaze = (pred_gaze == targets_gaze).sum().item()\n",
        "    accuracy_gaze = correct_gaze / targets_gaze.size(0)\n",
        "    train_gaze_accuracies.append(accuracy_gaze) # Store training accuracy\n",
        "\n",
        "    pred_laughter = torch.argmax(logits_laughter, dim=1)\n",
        "    correct_laughter = (pred_laughter == targets_laughter).sum().item()\n",
        "    accuracy_laughter = correct_laughter / targets_laughter.size(0)\n",
        "    train_laughter_accuracies.append(accuracy_laughter) # Store training accuracy\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        (val_logits_gaze, val_logits_laughter,\n",
        "         val_targets_gaze, val_targets_laughter) = model(X_validation, device)\n",
        "\n",
        "        val_loss_gaze = criterion(val_logits_gaze, val_targets_gaze)\n",
        "        val_loss_laughter = criterion(val_logits_laughter, val_targets_laughter)\n",
        "        val_total_loss = val_loss_gaze + val_loss_laughter\n",
        "        val_losses.append(val_total_loss.item()) # Store validation loss\n",
        "\n",
        "        val_pred_gaze = torch.argmax(val_logits_gaze, dim=1)\n",
        "        val_correct_gaze = (val_pred_gaze == val_targets_gaze).sum().item()\n",
        "        val_accuracy_gaze = val_correct_gaze / val_targets_gaze.size(0)\n",
        "        val_gaze_accuracies.append(val_accuracy_gaze) # Store validation accuracy\n",
        "\n",
        "        val_pred_laughter = torch.argmax(val_logits_laughter, dim=1)\n",
        "        val_correct_laughter = (val_pred_laughter == val_targets_laughter).sum().item()\n",
        "        val_accuracy_laughter = val_correct_laughter / val_targets_laughter.size(0)\n",
        "        val_laughter_accuracies.append(val_accuracy_laughter) # Store validation accuracy\n",
        "\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {total_loss:.4f}, Train Gaze Acc: {accuracy_gaze:.4f}, Train Laughter Acc: {accuracy_laughter:.4f}, \"\n",
        "          f\"Val Loss: {val_total_loss:.4f}, Val Gaze Acc: {val_accuracy_gaze:.4f}, Val Laughter Acc: {val_accuracy_laughter:.4f}\"\n",
        "          )\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    (test_logits_gaze, test_logits_laughter,\n",
        "     test_targets_gaze, test_targets_laughter) = model(X_test, device)\n",
        "\n",
        "    test_loss_gaze = criterion(test_logits_gaze, test_targets_gaze)\n",
        "    test_loss_laughter = criterion(test_logits_laughter, test_targets_laughter)\n",
        "    test_total_loss = test_loss_gaze + test_loss_laughter\n",
        "\n",
        "    test_pred_gaze = torch.argmax(test_logits_gaze, dim=1)\n",
        "    test_correct_gaze = (test_pred_gaze == test_targets_gaze).sum().item()\n",
        "    test_accuracy_gaze = test_correct_gaze / test_targets_gaze.size(0)\n",
        "\n",
        "    test_pred_laughter = torch.argmax(test_logits_laughter, dim=1)\n",
        "    test_correct_laughter = (test_pred_laughter == test_targets_laughter).sum().item()\n",
        "    test_accuracy_laughter = test_correct_laughter / test_targets_laughter.size(0)\n",
        "\n",
        "print(\"\\nTest Set Evaluation:\")\n",
        "print(f\"Test Loss: {test_total_loss:.4f}, Test Gaze Acc: {test_accuracy_gaze:.4f}, Test Laughter Acc: {test_accuracy_laughter:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL6QSXvOG3v7"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDF60x34G5YK"
      },
      "source": [
        "### 10-k CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56e9115f",
        "outputId": "d6ba6cdf-e6ce-4229-da9b-46997543cef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting 10-Fold Cross-Validation...\n",
            "\n",
            "--- Fold 1/10 ---\n",
            "Fold 1 - Test Loss: 2.0747, Test Gaze Acc: 0.3183, Test Laughter Acc: 0.6486\n",
            "\n",
            "--- Fold 2/10 ---\n",
            "Fold 2 - Test Loss: 1.9815, Test Gaze Acc: 0.5465, Test Laughter Acc: 0.6306\n",
            "\n",
            "--- Fold 3/10 ---\n",
            "Fold 3 - Test Loss: 1.9408, Test Gaze Acc: 0.5015, Test Laughter Acc: 0.7538\n",
            "\n",
            "--- Fold 4/10 ---\n",
            "Fold 4 - Test Loss: 2.0069, Test Gaze Acc: 0.4835, Test Laughter Acc: 0.6637\n",
            "\n",
            "--- Fold 5/10 ---\n",
            "Fold 5 - Test Loss: 2.1932, Test Gaze Acc: 0.4685, Test Laughter Acc: 0.6517\n",
            "\n",
            "--- Fold 6/10 ---\n",
            "Fold 6 - Test Loss: 2.0848, Test Gaze Acc: 0.3183, Test Laughter Acc: 0.6547\n",
            "\n",
            "--- Fold 7/10 ---\n",
            "Fold 7 - Test Loss: 2.0518, Test Gaze Acc: 0.5060, Test Laughter Acc: 0.6386\n",
            "\n",
            "--- Fold 8/10 ---\n",
            "Fold 8 - Test Loss: 2.0496, Test Gaze Acc: 0.4307, Test Laughter Acc: 0.6687\n",
            "\n",
            "--- Fold 9/10 ---\n",
            "Fold 9 - Test Loss: 2.1336, Test Gaze Acc: 0.4247, Test Laughter Acc: 0.6205\n",
            "\n",
            "--- Fold 10/10 ---\n",
            "Fold 10 - Test Loss: 2.0785, Test Gaze Acc: 0.4337, Test Laughter Acc: 0.7199\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "Average Test Loss: 2.0596\n",
            "Average Test Gaze Accuracy: 0.4432\n",
            "Average Test Laughter Accuracy: 0.6651\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "X_combined = X\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "test_losses = []\n",
        "test_gaze_accuracies = []\n",
        "test_laughter_accuracies = []\n",
        "\n",
        "print(\"Starting 10-Fold Cross-Validation...\")\n",
        "\n",
        "# Iterate over each fold\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X_combined)):\n",
        "    print(f\"\\n--- Fold {fold+1}/10 ---\")\n",
        "\n",
        "    # Split data into train and test sets for the current fold\n",
        "    X_train_fold = [X_combined[i] for i in train_index]\n",
        "    X_test_fold = [X_combined[i] for i in test_index]\n",
        "\n",
        "    # Initialize a new model for each fold to ensure independent training\n",
        "    model = SocialActionPredictor(\n",
        "        vocab_gaze, vocab_utterance, vocab_prosody,\n",
        "        vocab_facial, vocab_laughter, vocab_action,\n",
        "        vocab_laughter_chi, vocab_gaze_chi,\n",
        "        hidden_dim = 8\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop for the current fold\n",
        "    num_epochs_fold = 50\n",
        "    for epoch in range(num_epochs_fold):\n",
        "        model.train()\n",
        "        (logits_gaze, logits_laughter,\n",
        "         targets_gaze, targets_laughter) = model(X_train_fold, device)\n",
        "\n",
        "        loss_gaze = criterion(logits_gaze, targets_gaze)\n",
        "        loss_laughter = criterion(logits_laughter, targets_laughter)\n",
        "        loss = loss_gaze + loss_laughter\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on the test set for the current fold\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        (test_logits_gaze, test_logits_laughter,\n",
        "         test_targets_gaze, test_targets_laughter) = model(X_test_fold, device)\n",
        "\n",
        "        test_loss_gaze = criterion(test_logits_gaze, test_targets_gaze)\n",
        "        test_loss_laughter = criterion(test_logits_laughter, test_targets_laughter)\n",
        "        test_total_loss = test_loss_gaze + test_loss_laughter\n",
        "\n",
        "        test_pred_gaze = torch.argmax(test_logits_gaze, dim=1)\n",
        "        test_correct_gaze = (test_pred_gaze == test_targets_gaze).sum().item()\n",
        "        test_accuracy_gaze = test_correct_gaze / test_targets_gaze.size(0)\n",
        "\n",
        "        test_pred_laughter = torch.argmax(test_logits_laughter, dim=1)\n",
        "        test_correct_laughter = (test_pred_laughter == test_targets_laughter).sum().item()\n",
        "        test_accuracy_laughter = test_correct_laughter / test_targets_laughter.size(0)\n",
        "\n",
        "    test_losses.append(test_total_loss.item())\n",
        "    test_gaze_accuracies.append(test_accuracy_gaze)\n",
        "    test_laughter_accuracies.append(test_accuracy_laughter)\n",
        "\n",
        "    print(f\"Fold {fold+1} - Test Loss: {test_total_loss:.4f}, Test Gaze Acc: {test_accuracy_gaze:.4f}, Test Laughter Acc: {test_accuracy_laughter:.4f}\")\n",
        "\n",
        "# Report average performance across all folds\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Average Test Loss: {np.mean(test_losses):.4f}\")\n",
        "print(f\"Average Test Gaze Accuracy: {np.mean(test_gaze_accuracies):.4f}\")\n",
        "print(f\"Average Test Laughter Accuracy: {np.mean(test_laughter_accuracies):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4SXPv4rjMta"
      },
      "source": [
        "# Initial Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfro0itZAHfV"
      },
      "source": [
        "### Action and Gaze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Z-VGnADFM"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-y3TDjTDqit"
      },
      "outputs": [],
      "source": [
        "class SocialActionPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocabs,\n",
        "                 isGazeRelation,\n",
        "                 hidden_dim = 192,\n",
        "                 num_layers = 2,\n",
        "                 dropout = 0.3,\n",
        "                 input_size = 40,\n",
        "                 bidirectional = False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.isGazeRelation = isGazeRelation\n",
        "\n",
        "        # Vocabs\n",
        "        self.vocab_gaze = vocabs[\"gaze\"]\n",
        "        self.vocab_utterance = vocabs[\"utterance\"]\n",
        "        self.vocab_prosody = vocabs[\"prosody\"]\n",
        "        self.vocab_facial = vocabs[\"facial\"]\n",
        "        self.vocab_laughter = vocabs[\"laughter\"]\n",
        "        self.vocab_gaze_chi = vocabs[\"gaze_chi\"]\n",
        "        self.vocab_laughter_chi = vocabs[\"laughter_chi\"]\n",
        "\n",
        "        if self.isGazeRelation:\n",
        "            self.vocab_gazerelation = vocabs.vocab_gazerelation\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embed_utterance = nn.Embedding(len(self.vocab_utterance), 8)\n",
        "        self.embed_prosody = nn.Embedding(len(self.vocab_prosody), 4)\n",
        "        self.embed_facial = nn.Embedding(len(self.vocab_facial), 8)\n",
        "        self.embed_laughter = nn.Embedding(len(self.vocab_laughter), 4)\n",
        "        self.sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.embed_laughter_chi = nn.Embedding(len(self.vocab_laughter_chi), 4)\n",
        "\n",
        "        if self.isGazeRelation:\n",
        "            self.embed_gazepattern = nn.Embedding(len(self.vocab_gazerelation), 4)\n",
        "\n",
        "        # LSTM\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,    # so input shape is (batch, seq_len, input_dim)\n",
        "            dropout=dropout,     # optional dropout between layers\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Determine the input size for the decoder based on bidirectionality\n",
        "        decoder_input_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "        # Decoder to predict child gaze, laughter and mother's action\n",
        "        self.decoder_laughter = nn.Linear(decoder_input_dim, len(self.vocab_laughter_chi))\n",
        "        if self.isGazeRelation:\n",
        "            self.decoder_gazerelation = nn.Linear(decoder_input_dim, len(self.vocab_gazerelation))\n",
        "        self.project_gaze = nn.Linear(decoder_input_dim, self.sentence_encoder.get_sentence_embedding_dimension()) # Corrected input dimension\n",
        "\n",
        "    def lookup(self, vocab, value):\n",
        "        if value is None:\n",
        "            value = \"<UNK>\"\n",
        "        return vocab.get(value, vocab[\"<UNK>\"])\n",
        "\n",
        "    def encode_input(self, input, device):\n",
        "        gaze = torch.tensor([self.lookup(self.vocab_gaze, input.get(\"Gaze@MOT\"))]).to(device)\n",
        "        utterance = torch.tensor([self.lookup(self.vocab_utterance, input.get(\"Utterance@MOT\"))]).to(device)\n",
        "        prosody = torch.tensor([self.lookup(self.vocab_prosody, input.get(\"Prosody@MOT\"))]).to(device)\n",
        "        facial = torch.tensor([self.lookup(self.vocab_facial, input.get(\"Facial@MOT\"))]).to(device)\n",
        "        laughter = torch.tensor([self.lookup(self.vocab_laughter, input.get(\"Laughter@MOT\"))]).to(device)\n",
        "\n",
        "        action_text = input.get(\"Action@MOT\")\n",
        "\n",
        "        if action_text is None or not isinstance(action_text, str):\n",
        "            action_text = \"<UNK>\"\n",
        "        action = torch.tensor(self.sentence_encoder.encode(action_text)).to(device).unsqueeze(0)\n",
        "\n",
        "        gaze_text = input.get(\"Gaze@MOT\")\n",
        "\n",
        "        if gaze_text is None or not isinstance(gaze_text, str):\n",
        "            gaze_text = \"<UNK>\"\n",
        "        gaze = torch.tensor(self.sentence_encoder.encode(gaze_text)).to(device).unsqueeze(0)\n",
        "\n",
        "        return torch.cat([\n",
        "            gaze,\n",
        "            self.embed_utterance(utterance),\n",
        "            self.embed_prosody(prosody),\n",
        "            self.embed_facial(facial),\n",
        "            self.embed_laughter(laughter),\n",
        "            action\n",
        "        ], dim=-1)\n",
        "\n",
        "    def forward(self, sequence_of_dicts, device):\n",
        "        encoded_steps = []\n",
        "        targets_gaze_per_step = []\n",
        "        targets_laughter_per_step = []\n",
        "\n",
        "        if self.isGazeRelation:\n",
        "            targets_gazepattern_per_step = []\n",
        "\n",
        "        for step_dict in sequence_of_dicts:\n",
        "            encoded_steps.append(self.encode_input(step_dict, device))\n",
        "\n",
        "            gaze_chi_value = step_dict.get(\"Gaze@CHI\")\n",
        "\n",
        "            if gaze_chi_value is None or not isinstance(gaze_chi_value, str):\n",
        "                gaze_chi_value = \"<UNK>\"\n",
        "            gaze_chi_embedding = torch.tensor(self.sentence_encoder.encode(gaze_chi_value)).to(device).unsqueeze(0)\n",
        "            targets_gaze_per_step.append(gaze_chi_embedding.squeeze(0)) # Remove the batch dimension\n",
        "\n",
        "            targets_laughter_per_step.append(self.lookup(self.vocab_laughter_chi, step_dict.get(\"Laughter@CHI\")))\n",
        "\n",
        "            if self.isGazeRelation:\n",
        "                targets_gazepattern_per_step.append(self.lookup(self.vocab_gazepattern, step_dict.get(\"GazePattern\")))\n",
        "\n",
        "        x = torch.cat(encoded_steps, dim=0)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        predicted_gaze_embeddings = self.project_gaze(output[0])\n",
        "        logits_laughter_per_step = self.decoder_laughter(output[0])\n",
        "\n",
        "        targets_gaze_per_step = torch.stack(targets_gaze_per_step, dim=0)\n",
        "\n",
        "        targets_laughter_per_step = torch.tensor(targets_laughter_per_step, dtype=torch.long).to(device)\n",
        "\n",
        "        if self.isGazeRelation:\n",
        "            logits_gazepattern_per_step = self.decoder_gazepattern(output[0])\n",
        "            targets_gazepattern_per_step = torch.tensor(targets_gazepattern_per_step, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "            return (predicted_gaze_embeddings, logits_laughter_per_step, logits_gazepattern_per_step,\n",
        "            targets_gaze_per_step, targets_laughter_per_step, targets_gazepattern_per_step)\n",
        "\n",
        "        return (predicted_gaze_embeddings, logits_laughter_per_step,\n",
        "                targets_gaze_per_step, targets_laughter_per_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxhKeHAKLEM_"
      },
      "source": [
        "#### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opFQ94UCmMgI"
      },
      "outputs": [],
      "source": [
        "vocabs = {\"action\": vocab_action, \"gaze\": vocab_gaze, \"utterance\": vocab_utterance,\n",
        "             \"prosody\": vocab_prosody, \"facial\": vocab_facial, \"laughter\": vocab_laughter,\n",
        "             \"gazerelation\": vocab_gazerelation, \"laughter_chi\": vocab_laughter_chi, \"gaze_chi\": vocab_gaze_chi}\n",
        "\n",
        "isGazeRelation=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZuDTilaLGZI"
      },
      "outputs": [],
      "source": [
        "def training_loop(isGazeRelation, hidden_dim, num_layers, dropout, bidirectional, train_set, validation_set, test_set):\n",
        "        model = SocialActionPredictor(\n",
        "            vocabs = vocabs,\n",
        "            isGazeRelation = isGazeRelation,\n",
        "            hidden_dim = hidden_dim,\n",
        "            num_layers = num_layers,\n",
        "            dropout = dropout,\n",
        "            bidirectional = bidirectional,\n",
        "            input_size=792\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        criterion_emb = nn.CosineEmbeddingLoss()\n",
        "        cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "        num_epochs = 5\n",
        "\n",
        "        train_losses = []\n",
        "        train_gaze_similarities = [] # Store gaze similarities\n",
        "        train_laughter_accuracies = []\n",
        "        val_losses = []\n",
        "        val_gaze_similarities = [] # Store gaze similarities\n",
        "        val_laughter_accuracies = []\n",
        "\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            # Training\n",
        "            if isGazeRelation:\n",
        "                (predicted_gaze, logits_laughter, logits_gazerelation,\n",
        "                targets_gaze, targets_laughter, targets_gazerelation) = model(train_set, device)\n",
        "            else:\n",
        "                (predicted_gaze, logits_laughter,\n",
        "                targets_gaze, targets_laughter) = model(train_set, device)\n",
        "\n",
        "            similarity_target = torch.ones(predicted_gaze.size(0), device=device)  # all similar\n",
        "\n",
        "            loss_gaze = criterion_emb(predicted_gaze, targets_gaze, similarity_target)\n",
        "            loss_laughter = criterion(logits_laughter, targets_laughter)\n",
        "\n",
        "            if isGazeRelation:\n",
        "                loss_gazerelation = criterion(logits_gazerelation, targets_gazerelation)\n",
        "\n",
        "            # Combine losses\n",
        "            loss = loss_gaze + loss_laughter\n",
        "            if isGazeRelation:\n",
        "                loss += loss_gazerelation\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss = loss.item()\n",
        "            train_losses.append(total_loss) # Store training loss\n",
        "\n",
        "            # Calculate training gaze similarity\n",
        "            train_sim_gaze = cosine_similarity(predicted_gaze, targets_gaze).mean().item()\n",
        "            train_gaze_similarities.append(train_sim_gaze)\n",
        "\n",
        "            pred_laughter = torch.argmax(logits_laughter, dim=1)\n",
        "            correct_laughter = (pred_laughter == targets_laughter).sum().item()\n",
        "            accuracy_laughter = correct_laughter / targets_laughter.size(0)\n",
        "            train_laughter_accuracies.append(accuracy_laughter)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if isGazeRelation:\n",
        "                    (val_predicted_gaze, val_logits_laughter, val_logits_gazerelation,\n",
        "                    val_targets_gaze, val_targets_laughter, val_targets_gazerelation) = model(validation_set, device)\n",
        "                else:\n",
        "                    (val_predicted_gaze, val_logits_laughter,\n",
        "                    val_targets_gaze, val_targets_laughter) = model(validation_set, device)\n",
        "\n",
        "                val_loss_gaze = criterion_emb(val_predicted_gaze, val_targets_gaze, torch.ones(val_predicted_gaze.size(0), device=device))\n",
        "                val_loss_laughter = criterion(val_logits_laughter, val_targets_laughter)\n",
        "                if isGazeRelation:\n",
        "                    val_loss_gazerelation = criterion(val_logits_gazerelation, val_targets_gazerelation)\n",
        "                val_total_loss = val_loss_gaze + val_loss_laughter\n",
        "                if isGazeRelation:\n",
        "                    val_total_loss += val_loss_gazerelation\n",
        "                val_losses.append(val_total_loss.item()) # Store validation loss\n",
        "\n",
        "                # Calculate validation gaze similarity\n",
        "                val_sim_gaze = cosine_similarity(val_predicted_gaze, val_targets_gaze).mean().item()\n",
        "                val_gaze_similarities.append(val_sim_gaze)\n",
        "\n",
        "                val_pred_laughter = torch.argmax(val_logits_laughter, dim=1)\n",
        "                val_correct_laughter = (val_pred_laughter == val_targets_laughter).sum().item()\n",
        "                val_accuracy_laughter = val_correct_laughter / val_targets_laughter.size(0)\n",
        "                val_laughter_accuracies.append(val_accuracy_laughter) # Store validation accuracy\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                f\"Train Loss: {total_loss:.4f}, Train Gaze Sim: {train_sim_gaze:.4f}, Train Laughter Acc: {accuracy_laughter:.4f}, \"\n",
        "                f\"Val Loss: {val_total_loss:.4f}, Val Gaze Sim: {val_sim_gaze:.4f}, Val Laughter Acc: {val_accuracy_laughter:.4f}\"\n",
        "                )\n",
        "\n",
        "        print(\"Training finished.\")\n",
        "\n",
        "        # Evaluate on Test Set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isGazeRelation:\n",
        "                (test_predicted_gaze, test_logits_laughter, test_logits_gazerelation,\n",
        "                test_targets_gaze, test_targets_laughter, test_targets_gazerelation) = model(test_set, device)\n",
        "            else:\n",
        "                (test_predicted_gaze, test_logits_laughter,\n",
        "                test_targets_gaze, test_targets_laughter) = model(test_set, device)\n",
        "\n",
        "            test_loss_gaze = criterion_emb(test_predicted_gaze, test_targets_gaze, torch.ones(test_predicted_gaze.size(0), device=device))\n",
        "            test_loss_laughter = criterion(test_logits_laughter, test_targets_laughter)\n",
        "\n",
        "            if isGazeRelation:\n",
        "                test_loss_gazepattern = criterion(test_logits_gazerelation, test_targets_gazerelation)\n",
        "            test_total_loss = test_loss_gaze + test_loss_laughter\n",
        "\n",
        "            if isGazeRelation:\n",
        "                test_total_loss += test_loss_gazepattern\n",
        "\n",
        "            # Calculate test gaze similarity\n",
        "            test_sim_gaze = cosine_similarity(test_predicted_gaze, test_targets_gaze).mean().item()\n",
        "\n",
        "            test_pred_laughter = torch.argmax(test_logits_laughter, dim=1)\n",
        "            test_correct_laughter = (test_pred_laughter == test_targets_laughter).sum().item()\n",
        "            test_accuracy_laughter = test_correct_laughter / test_targets_laughter.size(0)\n",
        "\n",
        "\n",
        "        print(\"\\nTest Set Evaluation:\")\n",
        "        print(f\"Test Loss: {test_total_loss:.4f}, Test Gaze Sim: {test_sim_gaze:.4f}, Test Laughter Acc: {test_accuracy_laughter:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-jK912imMgJ",
        "outputId": "e18da42f-50a5-4a7c-b1c8-3ac353d25ef8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/elizaveta/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  20%|██        | 1/5 [02:00<08:02, 120.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Train Loss: 1.6834, Train Gaze Sim: 0.0268, Train Laughter Acc: 0.3406, Val Loss: 1.2487, Val Gaze Sim: 0.4919, Val Laughter Acc: 0.4315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  40%|████      | 2/5 [03:48<05:39, 113.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], Train Loss: 1.0844, Train Gaze Sim: 0.4917, Train Laughter Acc: 0.7296, Val Loss: 1.2479, Val Gaze Sim: 0.6688, Val Laughter Acc: 0.4315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  60%|██████    | 3/5 [05:39<03:44, 112.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], Train Loss: 0.8616, Train Gaze Sim: 0.6706, Train Laughter Acc: 0.7296, Val Loss: 1.1079, Val Gaze Sim: 0.7168, Val Laughter Acc: 0.4315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  80%|████████  | 4/5 [07:31<01:52, 112.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5], Train Loss: 0.7585, Train Gaze Sim: 0.7250, Train Laughter Acc: 0.7296, Val Loss: 1.0271, Val Gaze Sim: 0.7207, Val Laughter Acc: 0.5645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs: 100%|██████████| 5/5 [09:23<00:00, 112.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5], Train Loss: 0.7204, Train Gaze Sim: 0.7366, Train Laughter Acc: 0.7850, Val Loss: 1.1438, Val Gaze Sim: 0.7381, Val Laughter Acc: 0.5484\n",
            "Training finished.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Set Evaluation:\n",
            "Test Loss: 1.4615, Test Gaze Sim: 0.6234, Test Laughter Acc: 0.4154\n"
          ]
        }
      ],
      "source": [
        "train_set, validation_set, test_set = split_datset(X, 4, 0.75, 0.15)\n",
        "\n",
        "training_loop(isGazeRelation=False, hidden_dim=256, num_layers=1, dropout=0.3, bidirectional=False, train_set=train_set, validation_set=validation_set, test_set=test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bkiCUDduJvN"
      },
      "source": [
        "#### 10-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "d905b55ad37b41fc9f5df127d35c89fa",
            "f054c70032f447c78085fd0066562137",
            "a777b7156b4a4d22975a0af2da7bbcd0",
            "cdf9f5aa485c44eab98ea5a6b49090a1",
            "b527221f98e145d988b1c1242389f931",
            "44dc07288d8743dbbd8821cda4a8ed88",
            "f0dca152236a4c16a035626b31257834",
            "97d5cc9e450046668a34bfd3940a6099",
            "2a236b129dcb4e419a3c79d5a9eddc17",
            "a0c36aeb105d470391a4888b7134d138",
            "064ff7190ce2458097c58265d75ef7f7"
          ]
        },
        "id": "xAnT15P-uJe2",
        "outputId": "d9ccd059-5c94-4d3d-bd21-0b6246be240e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting 10-Fold Cross-Validation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d905b55ad37b41fc9f5df127d35c89fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Cross-Validation Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/10 ---\n",
            "Fold 1 - Test Loss: 1.4926, Test Gaze Sim: 0.8339, Test Laughter Acc: 0.7718, Test GazePattern Acc: 0.7147\n",
            "\n",
            "--- Fold 2/10 ---\n",
            "Fold 2 - Test Loss: 1.5404, Test Gaze Sim: 0.8182, Test Laughter Acc: 0.7898, Test GazePattern Acc: 0.6637\n",
            "\n",
            "--- Fold 3/10 ---\n",
            "Fold 3 - Test Loss: 1.5271, Test Gaze Sim: 0.8161, Test Laughter Acc: 0.7868, Test GazePattern Acc: 0.6697\n",
            "\n",
            "--- Fold 4/10 ---\n",
            "Fold 4 - Test Loss: 1.4678, Test Gaze Sim: 0.8330, Test Laughter Acc: 0.7778, Test GazePattern Acc: 0.6577\n",
            "\n",
            "--- Fold 5/10 ---\n",
            "Fold 5 - Test Loss: 1.4638, Test Gaze Sim: 0.8394, Test Laughter Acc: 0.8378, Test GazePattern Acc: 0.6907\n",
            "\n",
            "--- Fold 6/10 ---\n",
            "Fold 6 - Test Loss: 1.5554, Test Gaze Sim: 0.8236, Test Laughter Acc: 0.8078, Test GazePattern Acc: 0.6547\n",
            "\n",
            "--- Fold 7/10 ---\n",
            "Fold 7 - Test Loss: 1.4757, Test Gaze Sim: 0.8385, Test Laughter Acc: 0.7711, Test GazePattern Acc: 0.6928\n",
            "\n",
            "--- Fold 8/10 ---\n",
            "Fold 8 - Test Loss: 1.4921, Test Gaze Sim: 0.8135, Test Laughter Acc: 0.7922, Test GazePattern Acc: 0.6566\n",
            "\n",
            "--- Fold 9/10 ---\n",
            "Fold 9 - Test Loss: 1.6933, Test Gaze Sim: 0.8013, Test Laughter Acc: 0.7711, Test GazePattern Acc: 0.6355\n",
            "\n",
            "--- Fold 10/10 ---\n",
            "Fold 10 - Test Loss: 1.5160, Test Gaze Sim: 0.8258, Test Laughter Acc: 0.7560, Test GazePattern Acc: 0.6777\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "Average Test Loss: 1.5224\n",
            "Average Test Gaze Similarity: 0.8243\n",
            "Average Test Laughter Accuracy: 0.7862\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Combine train and test data for cross-validation\n",
        "X_combined = X\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "test_losses = []\n",
        "test_gaze_similarities = []\n",
        "test_laughter_accuracies = []\n",
        "\n",
        "print(\"Starting 10-Fold Cross-Validation...\")\n",
        "\n",
        "# Iterate over each fold\n",
        "for fold, (train_index, test_index) in tqdm(enumerate(kf.split(X_combined)), total=kf.get_n_splits(), desc=\"Cross-Validation Progress\"):\n",
        "    print(f\"\\n--- Fold {fold+1}/10 ---\")\n",
        "\n",
        "    # Split data into train and test sets for the current fold\n",
        "    X_train_fold = [X_combined[i] for i in train_index]\n",
        "    X_test_fold = [X_combined[i] for i in test_index]\n",
        "\n",
        "    # Initialize a new model for each fold to ensure independent training\n",
        "    model = SocialActionPredictor(\n",
        "        vocab_gaze, vocab_utterance, vocab_prosody,\n",
        "        vocab_facial, vocab_laughter, vocab_action,\n",
        "        vocab_laughter_chi, vocab_gaze_chi, vocab_gazepattern,\n",
        "        hidden_dim = 256, num_layers = 1,\n",
        "        dropout = 0.5,\n",
        "        bidirectional=True, input_size=792\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_emb = nn.CosineEmbeddingLoss()\n",
        "    cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    num_epochs_fold = 10\n",
        "    for epoch in range(num_epochs_fold):\n",
        "        model.train()\n",
        "        (predicted_gaze, logits_laughter, logits_gazepattern,\n",
        "         targets_gaze, targets_laughter, targets_gazepattern) = model(X_train_fold, device)\n",
        "\n",
        "        similarity_target = torch.ones(predicted_gaze.size(0), device=device)  # all similar\n",
        "\n",
        "        loss_gaze = criterion_emb(predicted_gaze, targets_gaze, similarity_target)\n",
        "        loss_laughter = criterion(logits_laughter, targets_laughter)\n",
        "        loss_gazepattern = criterion(logits_gazepattern, targets_gazepattern)\n",
        "        loss = loss_gaze + loss_laughter + loss_gazepattern\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on the test set for the current fold\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        (test_predicted_gaze, test_logits_laughter, test_logits_gazepattern,\n",
        "         test_targets_gaze, test_targets_laughter, test_targets_gazepattern) = model(X_test_fold, device)\n",
        "\n",
        "        test_loss_gaze = criterion_emb(test_predicted_gaze, test_targets_gaze, torch.ones(test_predicted_gaze.size(0), device=device))\n",
        "        test_loss_laughter = criterion(test_logits_laughter, test_targets_laughter)\n",
        "        test_loss_gazepattern = criterion(test_logits_gazepattern, test_targets_gazepattern)\n",
        "        test_total_loss = test_loss_gaze + test_loss_laughter + test_loss_gazepattern\n",
        "\n",
        "        # Calculate validation gaze similarity\n",
        "        test_sim_gaze = cosine_similarity(test_predicted_gaze, test_targets_gaze).mean().item()\n",
        "        # test_gaze_similarities.append(test_sim_gaze)\n",
        "\n",
        "        test_pred_laughter = torch.argmax(test_logits_laughter, dim=1)\n",
        "        test_correct_laughter = (test_pred_laughter == test_targets_laughter).sum().item()\n",
        "        test_accuracy_laughter = test_correct_laughter / test_targets_laughter.size(0)\n",
        "\n",
        "        test_pred_gazepattern = torch.argmax(test_logits_gazepattern, dim=1)\n",
        "        test_correct_gazepattern = (test_pred_gazepattern == test_targets_gazepattern).sum().item()\n",
        "        test_accuracy_gazepattern = test_correct_gazepattern / test_targets_gazepattern.size(0)\n",
        "\n",
        "\n",
        "    test_losses.append(test_total_loss.item())\n",
        "    test_gaze_similarities.append(test_sim_gaze)\n",
        "    test_laughter_accuracies.append(test_accuracy_laughter)\n",
        "\n",
        "    print(f\"Fold {fold+1} - Test Loss: {test_total_loss:.4f}, Test Gaze Sim: {test_sim_gaze:.4f}, Test Laughter Acc: {test_accuracy_laughter:.4f}, Test GazePattern Acc: {test_accuracy_gazepattern:.4f}\")\n",
        "\n",
        "# Report average performance across all folds\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Average Test Loss: {np.mean(test_losses):.4f}\")\n",
        "print(f\"Average Test Gaze Similarity: {np.mean(test_gaze_similarities):.4f}\")\n",
        "print(f\"Average Test Laughter Accuracy: {np.mean(test_laughter_accuracies):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q95l9eIhUWrT",
        "9OBHzIOlUbxz",
        "5v3UAUx1L5Yw",
        "L5x_Plz0Ufkb",
        "SL4_xX2KrVCx",
        "RaPXsLCx9KGI",
        "BfPVjz4S2SgL",
        "DxdpSP3SpA_t",
        "hc4ubVcflXbq",
        "sdhelgTrAILU",
        "MwIe2VYungUN",
        "h4LrOoYgGz7E"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "064ff7190ce2458097c58265d75ef7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a236b129dcb4e419a3c79d5a9eddc17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44dc07288d8743dbbd8821cda4a8ed88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97d5cc9e450046668a34bfd3940a6099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0c36aeb105d470391a4888b7134d138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a777b7156b4a4d22975a0af2da7bbcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d5cc9e450046668a34bfd3940a6099",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a236b129dcb4e419a3c79d5a9eddc17",
            "value": 10
          }
        },
        "b527221f98e145d988b1c1242389f931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf9f5aa485c44eab98ea5a6b49090a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c36aeb105d470391a4888b7134d138",
            "placeholder": "​",
            "style": "IPY_MODEL_064ff7190ce2458097c58265d75ef7f7",
            "value": " 10/10 [1:39:05&lt;00:00, 594.47s/it]"
          }
        },
        "d905b55ad37b41fc9f5df127d35c89fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f054c70032f447c78085fd0066562137",
              "IPY_MODEL_a777b7156b4a4d22975a0af2da7bbcd0",
              "IPY_MODEL_cdf9f5aa485c44eab98ea5a6b49090a1"
            ],
            "layout": "IPY_MODEL_b527221f98e145d988b1c1242389f931"
          }
        },
        "f054c70032f447c78085fd0066562137": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44dc07288d8743dbbd8821cda4a8ed88",
            "placeholder": "​",
            "style": "IPY_MODEL_f0dca152236a4c16a035626b31257834",
            "value": "Cross-Validation Progress: 100%"
          }
        },
        "f0dca152236a4c16a035626b31257834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
